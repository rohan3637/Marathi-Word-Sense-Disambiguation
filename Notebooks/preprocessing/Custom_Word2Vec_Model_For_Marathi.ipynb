{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom_Word2Vec_Model_For_Marathi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "8v50pzDxrTKd"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JR6Tgd7zvrl"
      },
      "source": [
        "## Extracting Sentences from data.txt file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbpT5SUAtEQq"
      },
      "source": [
        "DATA_TEXT_PATH = \"/content/drive/MyDrive/MARATHI_WSD_Data/Word_Net/data_txt.TXT\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9yGFvkD_c7z"
      },
      "source": [
        "all_sentences = []"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ywv6lmgtvXp"
      },
      "source": [
        "opened_file = open(DATA_TEXT_PATH, \"r\", encoding=\"utf-8-sig\")\n",
        "content = opened_file.read()\n",
        "\n",
        "content = content.split(\"\\n\")\n",
        "\n",
        "c = 0\n",
        "for line in content:\n",
        "    try:\n",
        "        sentence = line.split(\"|\")[1]\n",
        "        sentence1, sentence2 = sentence.split(\":\")\n",
        "        sentence1 = sentence1.strip().split(\" \")\n",
        "        sentence2  =sentence2.strip()\n",
        "        sentence2 = sentence2.replace('\"', \"\")\n",
        "        sentence2 = sentence2.replace(\".\", \" .\")\n",
        "        all_sentences.append(sentence1)\n",
        "        if \"/\" in sentence2:\n",
        "            for sent in sentence2.split(\"/\"):\n",
        "                all_sentences.append(sent.split(\" \"))\n",
        "        else:\n",
        "            all_sentences.append(sentence2.split(\" \"))\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfL1Qc19-Kte",
        "outputId": "ee4f9111-9683-4895-a291-0fb11f1cf862"
      },
      "source": [
        "len(all_sentences)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55504"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppXu3jgEvcA4",
        "outputId": "34eb05d2-7961-466d-880b-1431fb274a97"
      },
      "source": [
        "all_sentences[:5]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ज्यास', 'जन्म', 'नाही', 'असा'],\n",
              " ['ईश्वर', 'अजन्मा', 'आहे'],\n",
              " ['शुभ', 'नाही', 'असा'],\n",
              " ['या', 'योगामूळे', 'कुंडलीतील', 'इतर', 'अशुभ', 'योगांचा', 'नाश', 'होतो', '.'],\n",
              " ['ज्याने', 'प्रवेश', 'केला', 'नाही', 'असा']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwAd6fiSyB2d",
        "outputId": "3453e7be-7ea8-4b2f-8b92-d1d5a117c782"
      },
      "source": [
        "for sentence in all_sentences:\n",
        "    if \"जाते\" in sentence:\n",
        "        print(sentence)\n",
        "        break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ती', 'दर', 'सोमवारी', 'शिवालयात', 'जाते', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-oMnLGSz0ut"
      },
      "source": [
        "## Extracting Tourism Domain Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWRgqyCdyRIy"
      },
      "source": [
        "TOURISM_FILE_PATH1 = \"/content/drive/MyDrive/MARATHI_WSD_Data/2. MAR-TOURISM/MAR-TOURISM\"\n",
        "TOURISM_FILE_PATH2 = \"/content/drive/MyDrive/MARATHI_WSD_Data/Marathi Tourism Domain/Marathi Tourism Domain\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSv_U8oH0Gld"
      },
      "source": [
        "all_lines = []\n",
        "for i in os.listdir(TOURISM_FILE_PATH1):\n",
        "    if i!=\".ipynb_checkpoints\":\n",
        "        text_file = open(os.path.join(TOURISM_FILE_PATH1, i), \"r\", encoding=\"utf-8-sig\")\n",
        "        content = text_file.read()\n",
        "        content = content.split(\"\\n\")\n",
        "        lines = []\n",
        "        for i in content:\n",
        "            lines.append(i.split(\" \"))\n",
        "        line_words = []\n",
        "        for i in lines:\n",
        "            words = []\n",
        "            for j in i:\n",
        "                if \"#\" in j:\n",
        "                    words.append(j.split(\"#\"))\n",
        "            all_lines.append(words)\n",
        "            \n",
        "for i in os.listdir(TOURISM_FILE_PATH2):\n",
        "    if i!=\".ipynb_checkpoints\":\n",
        "        text_file = open(os.path.join(TOURISM_FILE_PATH2, i), \"r\", encoding=\"utf-8\")\n",
        "        content = text_file.read()\n",
        "        content = content.split(\"\\n\")\n",
        "        lines = []\n",
        "        for i in content:\n",
        "            lines.append(i.split(\" \"))\n",
        "        line_words = []\n",
        "        for i in lines:\n",
        "            words = []\n",
        "            for j in i:\n",
        "                if \"#\" in j:\n",
        "                    words.append(j.split(\"#\"))\n",
        "            all_lines.append(words)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZh-jPEt0TxV"
      },
      "source": [
        "all_lines_word_info = []\n",
        "for line in all_lines:\n",
        "    words_info = []\n",
        "    if line != []:\n",
        "        for word in line:\n",
        "            word_info = {}\n",
        "            word_info[\"word\"] = word[0]\n",
        "            if word[1] !=\"\" and word[1][0] == \"{\":\n",
        "                word_info[\"root_word\"] = word[1][1:len(word[1])-1].split(\",\")\n",
        "            else:\n",
        "                word_info[\"root_word\"] = word[1]\n",
        "            word_info[\"pos\"] = word[2]\n",
        "            word_info[\"word_position\"] = word[3]\n",
        "            word_info[\"word_id\"] = word[4]\n",
        "            words_info.append(word_info)\n",
        "        all_lines_word_info.append(words_info)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEmwm26elts0",
        "outputId": "72f6a0e1-8460-463e-cc45-672b5b1436f5"
      },
      "source": [
        "for line in all_lines_word_info:\n",
        "    print(line)\n",
        "    break"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'word': '\\ufeffचित्रगुप्त', 'root_word': '\\ufeffचित्रगुप्त', 'pos': 'n', 'word_position': 'w1', 'word_id': '26981'}, {'word': 'मंदिर', 'root_word': 'मंदिर', 'pos': 'n', 'word_position': 'w2', 'word_id': '452'}, {'word': 'हे', 'root_word': ['हा', 'हे'], 'pos': 'u', 'word_position': 'w3', 'word_id': '0'}, {'word': 'सूर्य', 'root_word': 'सूर्य', 'pos': 'n', 'word_position': 'w4', 'word_id': '2186'}, {'word': 'देवाचे', 'root_word': 'देव', 'pos': 'n', 'word_position': 'w5', 'word_id': '2484'}, {'word': 'मंदिर', 'root_word': 'मंदिर', 'pos': 'n', 'word_position': 'w6', 'word_id': '452'}, {'word': 'आहे', 'root_word': 'आह', 'pos': 'u', 'word_position': 'w7', 'word_id': '0'}, {'word': 'तसेच', 'root_word': 'तसे', 'pos': 'r', 'word_position': 'w8', 'word_id': '10811'}, {'word': 'विश्वनाथ', 'root_word': 'विश्वनाथ', 'pos': 'n', 'word_position': 'w9', 'word_id': '19174'}, {'word': 'मंदिर', 'root_word': 'मंदिर', 'pos': 'n', 'word_position': 'w10', 'word_id': '452'}, {'word': 'हे', 'root_word': ['हा', 'हे'], 'pos': 'u', 'word_position': 'w11', 'word_id': '0'}, {'word': 'शिवाचे', 'root_word': ['शिव', 'शीव'], 'pos': 'n', 'word_position': 'w12', 'word_id': '2061'}, {'word': 'आणि', 'root_word': 'आणि', 'pos': 'u', 'word_position': 'w13', 'word_id': '0'}, {'word': 'त्याचे', 'root_word': ['ते', 'तो'], 'pos': 'u', 'word_position': 'w14', 'word_id': '0'}, {'word': 'विश्वासू', 'root_word': ['विश्व', 'विश्वासू'], 'pos': 'a', 'word_position': 'w15', 'word_id': '248'}, {'word': 'वाहन', 'root_word': 'वाहन', 'pos': 'n', 'word_position': 'w16', 'word_id': '12919'}, {'word': 'असलेल्या', 'root_word': 'अस', 'pos': 'u', 'word_position': 'w17', 'word_id': '0'}, {'word': 'नंदी', 'root_word': 'नंद', 'pos': 'n', 'word_position': 'w18', 'word_id': '7974'}, {'word': 'ह्या', 'root_word': ['हा', 'ही', 'हे'], 'pos': 'u', 'word_position': 'w19', 'word_id': '0'}, {'word': 'बैलाचे', 'root_word': 'बैल', 'pos': 'n', 'word_position': 'w20', 'word_id': '4495'}, {'word': 'आहे', 'root_word': 'आह', 'pos': 'u', 'word_position': 'w21', 'word_id': '0'}, {'word': '.', 'root_word': '.', 'pos': 'u', 'word_position': 'w22', 'word_id': '0'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2S5auhSoHin",
        "outputId": "5a93782f-ccd7-4912-ee9c-3ee07243d03b"
      },
      "source": [
        "sentences = [[]]\n",
        "for word in all_lines_word_info[0]:\n",
        "    if type(word[\"root_word\"]) == list:\n",
        "        l_rword = len(word[\"root_word\"])\n",
        "        l_sen = len(sentences)\n",
        "        temp = sentences[-1].copy()\n",
        "        if l_sen < l_rword:\n",
        "            for _ in range(l_rword - l_sen):\n",
        "                sentences.append(sentences[0].copy())\n",
        "            for index in range(l_rword):\n",
        "                sentences[index].append(word[\"root_word\"][index])\n",
        "        else:\n",
        "            for index in range(l_sen):\n",
        "                try:\n",
        "                    sentences[index].append(word[\"root_word\"][index])\n",
        "                except:\n",
        "                    sentences[index].append(word[\"word\"])\n",
        "        if word[\"word\"] not in word[\"root_word\"]:\n",
        "            sentences.append(temp)\n",
        "            sentences[-1].append(word[\"word\"])\n",
        "    else:\n",
        "        if word[\"word\"] != word[\"root_word\"]:\n",
        "            sentences.append(sentences[-1].copy())\n",
        "        for sentence in sentences[:-1]:\n",
        "            sentence.append(word[\"root_word\"])\n",
        "        sentences[-1].append(word[\"word\"])\n",
        "t = sentences.copy()\n",
        "for sentence in sentences:\n",
        "    if t.count(sentence) > 1:\n",
        "        while t.count(sentence)!=1:\n",
        "            del t[t.index(sentence)]\n",
        "for sent in t:\n",
        "    print(' '.join(sent))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿चित्रगुप्त मंदिर हा सूर्य देव मंदिर आह तसे विश्वनाथ मंदिर हा शिव आणि ते विश्व वाहन अस नंद हा बैल आह .\n",
            "﻿चित्रगुप्त मंदिर हे सूर्य देव मंदिर आह तसे विश्वनाथ मंदिर हे शीव आणि तो विश्वासू वाहन अस नंद ही बैल आह .\n",
            "﻿चित्रगुप्त मंदिर हे सूर्य देवाचे मंदिर आह तसे विश्वनाथ मंदिर हे शिवाचे आणि त्याचे विश्वासू वाहन अस नंद हे बैल आह .\n",
            "﻿चित्रगुप्त मंदिर हे सूर्य देवाचे मंदिर आहे तसे विश्वनाथ मंदिर हे शिवाचे आणि त्याचे विश्वासू वाहन अस नंद ह्या बैल आह .\n",
            "﻿चित्रगुप्त मंदिर हे सूर्य देवाचे मंदिर आहे तसेच विश्वनाथ मंदिर हे शिवाचे आणि त्याचे विश्वासू वाहन अस नंद ह्या बैल आह .\n",
            "﻿चित्रगुप्त मंदिर हे सूर्य देवाचे मंदिर आहे तसेच विश्वनाथ मंदिर हे शिवाचे आणि त्याचे विश्वासू वाहन असलेल्या नंद ह्या बैल आह .\n",
            "﻿चित्रगुप्त मंदिर हे सूर्य देवाचे मंदिर आहे तसेच विश्वनाथ मंदिर हे शिवाचे आणि त्याचे विश्वासू वाहन असलेल्या नंदी ह्या बैल आह .\n",
            "﻿चित्रगुप्त मंदिर हे सूर्य देवाचे मंदिर आहे तसेच विश्वनाथ मंदिर हे शिवाचे आणि त्याचे विश्वासू वाहन असलेल्या नंदी ह्या बैलाचे आह .\n",
            "﻿चित्रगुप्त मंदिर हे सूर्य देवाचे मंदिर आहे तसेच विश्वनाथ मंदिर हे शिवाचे आणि त्याचे विश्वासू वाहन असलेल्या नंदी ह्या बैलाचे आहे .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ4ks0jk1Pg3"
      },
      "source": [
        "tourism_sentences = []\n",
        "\n",
        "for line in all_lines_word_info:\n",
        "    sentences = [[]]\n",
        "    for word in line:\n",
        "        if type(word[\"root_word\"]) == list:\n",
        "            l_rword = len(word[\"root_word\"])\n",
        "            l_sen = len(sentences)\n",
        "            temp = sentences[-1].copy()\n",
        "            if l_sen < l_rword:\n",
        "                for _ in range(l_rword - l_sen):\n",
        "                    sentences.append(sentences[0].copy())\n",
        "                for index in range(l_rword):\n",
        "                    sentences[index].append(word[\"root_word\"][index])\n",
        "            else:\n",
        "                for index in range(l_sen):\n",
        "                    try:\n",
        "                        sentences[index].append(word[\"root_word\"][index])\n",
        "                    except:\n",
        "                        sentences[index].append(word[\"word\"])\n",
        "            if word[\"word\"] not in word[\"root_word\"]:\n",
        "                sentences.append(temp)\n",
        "                sentences[-1].append(word[\"word\"])\n",
        "        else:\n",
        "            if word[\"word\"] != word[\"root_word\"]:\n",
        "                sentences.append(sentences[-1].copy())\n",
        "            for sentence in sentences[:-1]:\n",
        "                sentence.append(word[\"root_word\"])\n",
        "            sentences[-1].append(word[\"word\"])\n",
        "    t = sentences.copy()\n",
        "    for sentence in sentences:\n",
        "        if t.count(sentence) > 1:\n",
        "            while t.count(sentence)!=1:\n",
        "                del t[t.index(sentence)]\n",
        "\n",
        "    tourism_sentences.extend(t)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gimyVSn2j6_T",
        "outputId": "50e10349-b0d0-4e5e-c36a-3fce2a3d07f6"
      },
      "source": [
        "print(len(all_sentences))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "55504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzG90eR-1cZt"
      },
      "source": [
        "all_sentences = all_sentences + tourism_sentences"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FavO7ZUm1eNY",
        "outputId": "579a8154-42e2-4282-d574-7a3f1ccbf4a2"
      },
      "source": [
        "len(all_sentences)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "292220"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d22KFmZG15wG"
      },
      "source": [
        "## Extracting Health Domain Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Nbht0EL11t1"
      },
      "source": [
        "FILES_PATH1 = \"/content/drive/MyDrive/MARATHI_WSD_Data/3. MAR-HEALTH/MAR-HEALTH\"\n",
        "FILES_PATH2 = \"/content/drive/MyDrive/MARATHI_WSD_Data/Marathi Health Domain/Marathi Health Domain\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A73ORm2p2LDi"
      },
      "source": [
        "all_lines = []\n",
        "for i in os.listdir(FILES_PATH1):\n",
        "    if i!=\".ipynb_checkpoints\":\n",
        "        text_file = open(os.path.join(FILES_PATH1, i), \"r\", encoding=\"utf-8-sig\")\n",
        "        content = text_file.read()\n",
        "        content = content.split(\"\\n\")\n",
        "        lines = []\n",
        "        for i in content:\n",
        "            lines.append(i.split(\" \"))\n",
        "        line_words = []\n",
        "        for i in lines:\n",
        "            words = []\n",
        "            for j in i:\n",
        "                if \"#\" in j:\n",
        "                    words.append(j.split(\"#\"))\n",
        "            all_lines.append(words)\n",
        "            \n",
        "for i in os.listdir(FILES_PATH2):\n",
        "    if i!=\".ipynb_checkpoints\":\n",
        "        text_file = open(os.path.join(FILES_PATH2, i), \"r\", encoding=\"utf-8\")\n",
        "        content = text_file.read()\n",
        "        content = content.split(\"\\n\")\n",
        "        lines = []\n",
        "        for i in content:\n",
        "            lines.append(i.split(\" \"))\n",
        "        line_words = []\n",
        "        for i in lines:\n",
        "            words = []\n",
        "            for j in i:\n",
        "                if \"#\" in j:\n",
        "                    words.append(j.split(\"#\"))\n",
        "            all_lines.append(words)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ba891W7d2Nkg"
      },
      "source": [
        "all_lines_word_info = []\n",
        "for line in all_lines:\n",
        "    words_info = []\n",
        "    if line != []:\n",
        "        for word in line:\n",
        "            word_info = {}\n",
        "            word_info[\"word\"] = word[0]\n",
        "            if word[1] !=\"\" and word[1][0] == \"{\":\n",
        "                word_info[\"root_word\"] = word[1][1:len(word[1])-1].split(\",\")\n",
        "            else:\n",
        "                word_info[\"root_word\"] = word[1]\n",
        "            word_info[\"pos\"] = word[2]\n",
        "            word_info[\"word_position\"] = word[3]\n",
        "            word_info[\"word_id\"] = word[4]\n",
        "            words_info.append(word_info)\n",
        "        all_lines_word_info.append(words_info)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX-Ocaep0Hvm",
        "outputId": "04ebb11d-7de6-41bd-ea80-4617d6b64fb9"
      },
      "source": [
        "sentences = [[]]\n",
        "for word in all_lines_word_info[3]:\n",
        "    if type(word[\"root_word\"]) == list:\n",
        "        l_rword = len(word[\"root_word\"])\n",
        "        l_sen = len(sentences)\n",
        "        temp = sentences[-1].copy()\n",
        "        if l_sen < l_rword:\n",
        "            for _ in range(l_rword - l_sen):\n",
        "                sentences.append(sentences[0].copy())\n",
        "            for index in range(l_rword):\n",
        "                sentences[index].append(word[\"root_word\"][index])\n",
        "        else:\n",
        "            for index in range(l_sen):\n",
        "                try:\n",
        "                    sentences[index].append(word[\"root_word\"][index])\n",
        "                except:\n",
        "                    sentences[index].append(word[\"word\"])\n",
        "        if word[\"word\"] not in word[\"root_word\"]:\n",
        "            sentences.append(temp)\n",
        "            sentences[-1].append(word[\"word\"])\n",
        "    else:\n",
        "        if word[\"word\"] != word[\"root_word\"]:\n",
        "            sentences.append(sentences[-1].copy())\n",
        "        for sentence in sentences[:-1]:\n",
        "            sentence.append(word[\"root_word\"])\n",
        "        sentences[-1].append(word[\"word\"])\n",
        "t = sentences.copy()\n",
        "for sentence in sentences:\n",
        "    if t.count(sentence) > 1:\n",
        "        while t.count(sentence)!=1:\n",
        "            del t[t.index(sentence)]\n",
        "for sent in t:\n",
        "    print(' '.join(sent))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "काही कर येणे ऊन अस , कधी नस हा मान्य आह , पण अस \" काहीच कर ये नस \" हा जेव्हा आवश्यक अस तेव्हा त एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता ये नाह असा , कधी नस हे मान्य आह , पण असा \" काहीच कर ये नस \" हे जेव्हा आवश्यक अस तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधी नस हे मान्य आह , पण असे \" काहीच कर ये नस \" हे जेव्हा आवश्यक अस तेव्हा ते एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधीच नस हे मान्य आह , पण असे \" काहीच कर ये नस \" हे जेव्हा आवश्यक अस तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधीच नसते हे मान्य आह , पण असे \" काहीच कर ये नस \" हे जेव्हा आवश्यक अस तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधीच नसते हे मान्य आहे , पण असे \" काहीच कर ये नस \" हे जेव्हा आवश्यक अस तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधीच नसते हे मान्य आहे , पण असे \" काहीच करता ये नस \" हे जेव्हा आवश्यक अस तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधीच नसते हे मान्य आहे , पण असे \" काहीच करता येत नस \" हे जेव्हा आवश्यक अस तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधीच नसते हे मान्य आहे , पण असे \" काहीच करता येत नसणे \" हे जेव्हा आवश्यक अस तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधीच नसते हे मान्य आहे , पण असे \" काहीच करता येत नसणे \" हे जेव्हा आवश्यक असेल तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक अस .\n",
            "काही करता येत नाही असे , कधीच नसते हे मान्य आहे , पण असे \" काहीच करता येत नसणे \" हे जेव्हा आवश्यक असेल तेव्हा ती एक पवित्र जेफर्सनिक फसवणूक असावी .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf6xyJ2q0cVA"
      },
      "source": [
        "health_sentences = []\n",
        "\n",
        "for line in all_lines_word_info:\n",
        "    sentences = [[]]\n",
        "    for word in line:\n",
        "        if type(word[\"root_word\"]) == list:\n",
        "            l_rword = len(word[\"root_word\"])\n",
        "            l_sen = len(sentences)\n",
        "            temp = sentences[-1].copy()\n",
        "            if l_sen < l_rword:\n",
        "                for _ in range(l_rword - l_sen):\n",
        "                    sentences.append(sentences[0].copy())\n",
        "                for index in range(l_rword):\n",
        "                    sentences[index].append(word[\"root_word\"][index])\n",
        "            else:\n",
        "                for index in range(l_sen):\n",
        "                    try:\n",
        "                        sentences[index].append(word[\"root_word\"][index])\n",
        "                    except:\n",
        "                        sentences[index].append(word[\"word\"])\n",
        "            if word[\"word\"] not in word[\"root_word\"]:\n",
        "                sentences.append(temp)\n",
        "                sentences[-1].append(word[\"word\"])\n",
        "        else:\n",
        "            if word[\"word\"] != word[\"root_word\"]:\n",
        "                sentences.append(sentences[-1].copy())\n",
        "            for sentence in sentences[:-1]:\n",
        "                sentence.append(word[\"root_word\"])\n",
        "            sentences[-1].append(word[\"word\"])\n",
        "    t = sentences.copy()\n",
        "    for sentence in sentences:\n",
        "        if t.count(sentence) > 1:\n",
        "            while t.count(sentence)!=1:\n",
        "                del t[t.index(sentence)]\n",
        "\n",
        "    health_sentences.extend(t)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFRd6BeK2ks-",
        "outputId": "78f75edb-d831-4e25-ad26-a71bb1aa472c"
      },
      "source": [
        "print(len(health_sentences), len(all_sentences))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "95890 292220\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kIhMqMJ2m0m"
      },
      "source": [
        "all_sentences = all_sentences + health_sentences"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfZ1-yms2qHt",
        "outputId": "d7dd2dd6-727f-4777-f028-9423cdd7215b"
      },
      "source": [
        "len(all_sentences)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "388110"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZOxw1J-26Sl"
      },
      "source": [
        "## Extracting News Domain Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_APoZR020PI"
      },
      "source": [
        "FILE_PATH = \"/content/drive/MyDrive/MARATHI_WSD_Data/Marathi News Domain/new_text_marathi.TXT\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNWw3r--3G3X"
      },
      "source": [
        "opened_file = open(FILE_PATH, \"r\", encoding=\"utf-8-sig\")\n",
        "content = opened_file.read()"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2L67Wkx3UBG"
      },
      "source": [
        "sentences_with_pos = []\n",
        "for i in content.split(\"\\n\"):\n",
        "    if len(i)!=0 and i[0] != \"<\":\n",
        "        sentences_with_pos.append(i)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4T5g80U3Xim"
      },
      "source": [
        "news_sentences = []\n",
        "for sentence in sentences_with_pos:\n",
        "    sent = []\n",
        "    for word in sentence.split(\" \"):\n",
        "        x = word.split(\"_\")\n",
        "        if len(x)!=1:\n",
        "            sent.append(word.split(\"_\")[0])\n",
        "    news_sentences.append(sent)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gC6S_aOo3Y_n",
        "outputId": "07818783-7d88-487e-b520-eb51d40bbe6f"
      },
      "source": [
        "len(news_sentences)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "957"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNHiyK2m3nCI"
      },
      "source": [
        "all_sentences = all_sentences + news_sentences"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-szWHKM3vRx",
        "outputId": "560b5695-4ee2-4f60-b69b-0a684bf7df35"
      },
      "source": [
        "print(len(all_sentences))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "389067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqrblqb8OYQ0"
      },
      "source": [
        "DATA_TEXT_PATH = \"/content/drive/MyDrive/MARATHI_WSD_Data/Word_Net/data_txt.TXT\"\n",
        "\n",
        "opened_file = open(DATA_TEXT_PATH, \"r\", encoding=\"utf-8-sig\")\n",
        "content = opened_file.read()\n",
        "\n",
        "content = content.split(\"\\n\")\n",
        "\n",
        "true_pairs = []\n",
        "for line in content:\n",
        "    try:\n",
        "        sentence = line.split(\"|\")[0].strip()\n",
        "        words = sentence.split(\" \")[3]\n",
        "        if \":\" in words:\n",
        "            words = words.split(\":\")\n",
        "            true_pairs.append(words)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzTC_j-CwxZi",
        "outputId": "9a3aeb2a-8747-4c4f-c889-dcbd0e64c243"
      },
      "source": [
        "print(true_pairs[:5])"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['अशुभ', 'अमंगळ'], ['पुण्यभूमी', 'पवित्रभूमी', 'पुण्यस्थान', 'पवित्रस्थान', 'पावनस्थान'], ['शिवालय', 'शिवमंदिर'], ['आलेला', 'आगत'], ['जन्मलेला', 'जलमलेला', 'जल्मलेला']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIaj9btd1pK7",
        "outputId": "97c596d3-df80-46da-8054-1d125bcca6e6"
      },
      "source": [
        "for pair in true_pairs:\n",
        "    if \"जाते\" in pair:\n",
        "        print(pair)\n",
        "        break"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['जाते', 'घट्टी', 'चक्की']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A26_kWmrxmAK",
        "outputId": "2499a2f1-b183-437f-bfb8-35af6a76d6c6"
      },
      "source": [
        "pairs = true_pairs[:200]\n",
        "len(pairs)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv1YE89RzIQM"
      },
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "def score(pairs, model):\n",
        "    score = 0\n",
        "    count = 0\n",
        "    for pair in pairs:\n",
        "        try:\n",
        "            dist = cosine(model[pair[0]], model[pair[1]])\n",
        "            # dist = model.wv.similarity(pair[0], pair[1])\n",
        "            score += dist\n",
        "            count += 1\n",
        "        except:\n",
        "            pass\n",
        "    return score / count"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ekd_CxrG3zM1"
      },
      "source": [
        "## Training a Word2vec Model on all the extracted Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQi4xDo23xNf"
      },
      "source": [
        "word2vec_model1 = Word2Vec(all_sentences, min_count=1, workers=3, sg = 0, size=200)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5K4d_Qr0llf"
      },
      "source": [
        "# word2vec_model2 = Word2Vec(all_sentences, min_count=1, workers=3, sg = 1, size=100)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-Savb0h-y5y",
        "outputId": "dc7566e9-e55d-43a5-d720-be306c0806e2"
      },
      "source": [
        "word2vec_model1.similar_by_word(\"उत्पादित\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('प्रदर्शित', 0.5822898149490356),\n",
              " ('कुंचन', 0.5771068334579468),\n",
              " ('फॅशन', 0.5691759586334229),\n",
              " ('नांबियार', 0.5564556121826172),\n",
              " ('बेलगुंठाला', 0.5511257648468018),\n",
              " ('विक्री', 0.5371506214141846),\n",
              " ('सुशोभीत', 0.5349873900413513),\n",
              " ('प्रेरित', 0.5289438962936401),\n",
              " ('सोहळ्याने', 0.5196452140808105),\n",
              " ('हल्ल्यांच्या', 0.5171300172805786)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFPb_eA7KIKn"
      },
      "source": [
        "## Word Sense Disambiguation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxQ5Pbap71l2"
      },
      "source": [
        "DATA_TEXT_PATH = \"/content/drive/MyDrive/MARATHI_WSD_Data/Word_Net/data_txt.TXT\"\n",
        "IDX_NOUN_PATH = \"/content/drive/MyDrive/MARATHI_WSD_Data/Word_Net/idxnoun_txt.TXT\"\n",
        "IDX_PATH = \"/content/drive/MyDrive/MARATHI_WSD_Data/Word_Net/index_txt.TXT\""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjKhjFTCKnMF"
      },
      "source": [
        "opened_file = open(IDX_NOUN_PATH, \"r\", encoding=\"utf-8-sig\")\n",
        "idx_noun_content = opened_file.read()\n",
        "\n",
        "noun_content = idx_noun_content.split(\"\\n\")\n",
        "\n",
        "nouns = []\n",
        "for noun in noun_content:\n",
        "    text = noun.split(\" \")\n",
        "    try:\n",
        "        skip1 = int(text[2])\n",
        "        skip2 = skip1+1\n",
        "        nouns.append((text[0], text[2+skip2+1:]))\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cez5SaREKtoy",
        "outputId": "1a36f46e-94bf-4f1a-a416-acbbde2ca31b"
      },
      "source": [
        "nouns[:5]"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('-तरंग', ['00005942']),\n",
              " ('-धारा', ['00027382']),\n",
              " ('-सारखा', ['00026718']),\n",
              " ('1000000', ['00031077']),\n",
              " ('10000000', ['00015838'])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXX_Cod5L8ua",
        "outputId": "9a43b9fd-1ac6-417e-e1cd-435c857f0d30"
      },
      "source": [
        "for noun in nouns:\n",
        "    if noun[0] == \"गिनीपिग\":\n",
        "        print(noun)\n",
        "        break"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('गिनीपिग', ['00020220'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AEpWwn9z8Ro",
        "outputId": "b0c75326-2ced-4410-89d8-4a391d93bc40"
      },
      "source": [
        "for noun in nouns:\n",
        "    if noun[0] == \"जाते\":\n",
        "        print(noun)\n",
        "        break"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('जाते', ['00012325'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p9-JMWsKyQw"
      },
      "source": [
        "import difflib\n",
        "\n",
        "opened_file = open(DATA_TEXT_PATH, \"r\", encoding=\"utf-8-sig\")\n",
        "data_content = opened_file.read()\n",
        "\n",
        "data_content = data_content.split(\"\\n\")\n",
        "\n",
        "all_syns = []\n",
        "for syn in data_content:\n",
        "    try:\n",
        "        ids, sentence = syn.split(\" | \")\n",
        "        x = ids.split(\" \")\n",
        "        all_syns.append((x[0], x[3].split(\":\"), sentence))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "nouns_data = []\n",
        "for noun in nouns:\n",
        "    for syn_id in noun[1]:\n",
        "        for syn in all_syns:\n",
        "            if syn_id == syn[0]:\n",
        "                if \":\" in syn[2]:\n",
        "                    sentence = syn[2].split(\":\")[1:]\n",
        "                elif \"::\" in syn[2]:\n",
        "                    sentence = syn[2].split(\"::\")[1:]\n",
        "                sentence = ' '.join(sentence)\n",
        "                sentence = sentence.replace('\"',\"\")\n",
        "                sentence = sentence.replace('.',\"\")\n",
        "                sentence = sentence.split(\" \")\n",
        "                flag = False\n",
        "                for synonym in syn[1]:\n",
        "                    match = difflib.get_close_matches(synonym, sentence)\n",
        "                    if len(match)!=0:\n",
        "                        flag = True\n",
        "                        sentence[sentence.index(match[0])] = noun[0]\n",
        "                        break\n",
        "                if flag:\n",
        "                    nouns_data.append((' '.join(sentence), noun[0], syn_id))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E04iznDuNYjr",
        "outputId": "02fd83c0-c129-49f3-9e28-4e01af5169d0"
      },
      "source": [
        "for syn in all_syns:\n",
        "    if syn[0] == \"00020220\":\n",
        "        print(syn)\n",
        "        break"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('00020220', ['गिनीपिग'], 'शेपुट नसलेला उंदरासारखा एक सस्तन प्राणी:\"गिनीपिगचे मांस लोक खातात.\"')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IdpV3Iw2UWX",
        "outputId": "f1c52573-3709-49bc-ea42-9b31c8f03f8b"
      },
      "source": [
        "for syn in all_syns:\n",
        "    if \"जाते\" in syn[1]:\n",
        "        print(syn)\n",
        "        break"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('00012325', ['जाते', 'घट्टी', 'चक्की'], 'दोन बसके वर्तुळाकार दगड एकमेकांवर घासून मधल्या पदार्थाचा चुराडा करतील अशी योजना ज्यात असते ते दळाण्याचे साधन\"आई सकाळी जाते घेऊन दळायला बसायची\"')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7EYYCDtMvOj",
        "outputId": "905ac3f5-a3f9-4301-e4c3-4823d9db952e"
      },
      "source": [
        "for noun in nouns_data:\n",
        "    if noun[1] == \"गिनीपिग\":\n",
        "        print(noun)\n",
        "        break"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('गिनीपिग मांस लोक खातात', 'गिनीपिग', '00020220')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGdeZDlw3QQW",
        "outputId": "8adf7f2e-a252-418a-9716-5b547a49b8a7"
      },
      "source": [
        "nouns_data_words = []\n",
        "for noun in nouns_data:\n",
        "    nouns_data_words.append(noun[1])\n",
        "len(nouns_data), len(nouns_data_words)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(34237, 34237)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWx6A5lJ24oe"
      },
      "source": [
        "new_words_list = []\n",
        "for pair in true_pairs:\n",
        "    for word in pair:\n",
        "        if word not in nouns_data_words:\n",
        "            t = pair.copy()\n",
        "            t.remove(word)\n",
        "            for temp_word in t:\n",
        "                if temp_word in nouns_data_words:\n",
        "                    for t_noun in nouns_data:\n",
        "                        if t_noun[1] == temp_word:\n",
        "                            ans = list(t_noun)\n",
        "                            ans[0] = ans[0].replace(temp_word, word)\n",
        "                            ans[1] = word\n",
        "                            new_words_list.append(tuple(ans))"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6m613DY-5UMn",
        "outputId": "afdc95d6-9f24-432d-806e-b5655f0f6b0a"
      },
      "source": [
        "len(new_words_list)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3988"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HbWJ73u5Wqi",
        "outputId": "67bbddfe-7706-4b32-e61c-708792f5c495"
      },
      "source": [
        "for noun in new_words_list:\n",
        "    if noun[1] == \"जाते\":\n",
        "        print(noun)\n",
        "        break"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('जाते तिला चालायला त्रास होतो आहे', 'जाते', '00012325')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQqz_1o25WtN",
        "outputId": "b619c6a8-b327-49fb-925c-34de22faa0c0"
      },
      "source": [
        "all_nouns_data = nouns_data + new_words_list\n",
        "len(all_nouns_data)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38225"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSIfJzrn5WwH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2HVsK-p0Jzo",
        "outputId": "98545a8a-dd32-4339-8cdc-7764644cdf55"
      },
      "source": [
        "for noun in all_nouns_data:\n",
        "    if noun[1] == \"जाते\":\n",
        "        print(noun)\n",
        "        break"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('जाते तिला चालायला त्रास होतो आहे', 'जाते', '00012325')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCrED54uLAQm"
      },
      "source": [
        "noun_sent_data = {}\n",
        "for i in all_nouns_data:\n",
        "    try:\n",
        "        if len(i[0].split()) != 1:\n",
        "            if \"::\" in i[0]:\n",
        "                meaning, sentence = i[0].split(\"::\")\n",
        "            elif \":\" in i[0]:\n",
        "                meaning, sentence = i[0].split(\":\")\n",
        "            else:\n",
        "                meaning, sentence = i[0].split(\".\")\n",
        "            sentence = sentence[1:len(sentence)-1]\n",
        "    except:\n",
        "        if '\"' not in i[0]:\n",
        "            sentence = i[0]\n",
        "        else:\n",
        "            start = i[0].index('\"')\n",
        "            end = len(i[0])\n",
        "            sentence = i[0][start+1:end-1]\n",
        "    s = sentence.split(\"/\")\n",
        "    if i[1] not in noun_sent_data:\n",
        "        noun_sent_data[i[1]] = {i[2]:[]}\n",
        "        for sent in s:\n",
        "            if i[1] in sent:\n",
        "                sent = sent.replace('\"', \"\")\n",
        "                noun_sent_data[i[1]][i[2]].append(sent)\n",
        "    else:\n",
        "        noun_sent_data[i[1]][i[2]] = []\n",
        "        for sent in s:\n",
        "            if i[1] in sent:\n",
        "                sent = sent.replace('\"', \"\")\n",
        "                noun_sent_data[i[1]][i[2]].append(sent)\n",
        "    if len(noun_sent_data[i[1]]) == 0:\n",
        "        del noun_sent_data[i[1]]\n",
        "    if noun_sent_data[i[1]][i[2]] == []:\n",
        "        del noun_sent_data[i[1]][i[2]]"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AXE-9YIMLfx",
        "outputId": "ebbf3b06-c340-4efa-9ae6-f827a98e6d75"
      },
      "source": [
        "noun_sent_data[\"गिनीपिग\"]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'00020220': ['गिनीपिग मांस लोक खातात']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCgHTJ7QMLjD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5591697-36e8-4228-e35b-46836e9b0cd4"
      },
      "source": [
        "noun_sent_data[\"जाते\"]"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'00012325': ['जाते जाते तुटला'], '00012326': ['जाते पट्टा तुटला']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XEC7OezMLn7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nayHC_gkMLqi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNldYajrWmfH",
        "outputId": "685cfe2f-9695-4ec0-b6a3-0034764d8b77"
      },
      "source": [
        "noun_sent_data[\"अंग\"]"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'00001954': ['शरीरातील प्रत्येक अंग महत्त्वाचे असते'],\n",
              " '00001957': ['या वस्तूचा प्रत्येक अंग स्वदेशी आहे '],\n",
              " '00001961': ['आपले अंग पाच महाभूतांपासून बनलेले आहे'],\n",
              " '00003388': ['क्रिकेटाच्या खेळात सचिनचे अंग जगजाहीर आहे ',\n",
              "  ' त्याला गायनाचेही अंग आहे'],\n",
              " '00003966': ['माझी उजवी अंग दुखते आहे'],\n",
              " '00010208': ['ह्या कामात मोठ्या भावाचा अंग आहे'],\n",
              " '00037159': ['दुर्योधनाने कर्णाला अंग दिला']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJJjOsNkL8GK",
        "outputId": "78168ae7-0a48-4ec7-e6de-cf71d73172f6"
      },
      "source": [
        "noun_vectors = {}\n",
        "\n",
        "for noun in noun_sent_data:\n",
        "    noun_vectors[noun] = {}\n",
        "\n",
        "    for nid in noun_sent_data[noun]:\n",
        "        for sentence in noun_sent_data[noun][nid]:\n",
        "            sense_vector = 0\n",
        "            count = 0\n",
        "            tokens = sentence.split(\" \")\n",
        "\n",
        "            noun_count = tokens.count(noun)\n",
        "\n",
        "            if noun_count > 1:\n",
        "                position = 1\n",
        "\n",
        "                for i in range(1, noun_count + 1):\n",
        "                    count = 0\n",
        "                    sense_vector = 0\n",
        "                    for word in tokens:\n",
        "                        try:\n",
        "                            if word != noun:\n",
        "                                sense_vector += word2vec_model1[word]\n",
        "                                count+=1\n",
        "                            else:\n",
        "                                if position != i:\n",
        "                                    sense_vector += word2vec_model1[word]\n",
        "                                    count += 1\n",
        "                                position += 1\n",
        "                        except:\n",
        "                            pass\n",
        "\n",
        "                    # print(len(sense_vector))\n",
        "                    if count != 0:\n",
        "                        if nid not in noun_vectors[noun]:\n",
        "                            noun_vectors[noun][nid] = []\n",
        "                            noun_vectors[noun][nid].append(sense_vector / count)\n",
        "                        else:\n",
        "                            noun_vectors[noun][nid].append(sense_vector / count)\n",
        "\n",
        "            else:\n",
        "                for word in tokens:\n",
        "                    try:\n",
        "                        if noun != word:\n",
        "                            sense_vector += word2vec_model1[word]\n",
        "                            count+=1\n",
        "                    except:\n",
        "                        pass\n",
        "                # print(len(sense_vector))\n",
        "                if count != 0:\n",
        "                    if nid not in noun_vectors[noun]:\n",
        "                        noun_vectors[noun][nid] = []\n",
        "                        noun_vectors[noun][nid].append(sense_vector / count)\n",
        "                    else:\n",
        "                        noun_vectors[noun][nid].append(sense_vector / count)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:45: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S27JmOJXR15u"
      },
      "source": [
        "import math\n",
        "def cosine_similarity(v1,v2):\n",
        "    sumxx, sumxy, sumyy = 0, 0, 0\n",
        "    for i in range(len(v1)):\n",
        "        x = v1[i]; y = v2[i]\n",
        "        sumxx += x*x\n",
        "        sumyy += y*y\n",
        "        sumxy += x*y\n",
        "    return sumxy/math.sqrt(sumxx*sumyy)\n",
        "\n",
        "def getSense(sentence, noun, noun_position = 1, noun_vectors = noun_vectors):\n",
        "    sentence = sentence.strip().replace(\".\", \" .\").split(\" \")\n",
        "    # print(sentence)\n",
        "\n",
        "    sense_vector = 0\n",
        "    count = 0\n",
        "    position = 1\n",
        "    for word in sentence:\n",
        "\n",
        "        if noun!=word:\n",
        "            sense_vector += word2vec_model1[word]\n",
        "            count += 1\n",
        "        else:\n",
        "            if position != noun_position:\n",
        "                sense_vector += word2vec_model1[word]\n",
        "                count += 1\n",
        "            position += 1\n",
        "\n",
        "    \n",
        "    if noun in noun_vectors:\n",
        "        cosines = {}\n",
        "        for nid in noun_vectors[noun]:\n",
        "            c = []\n",
        "            for vector in noun_vectors[noun][nid]:\n",
        "                c.append(cosine_similarity(vector, sense_vector))\n",
        "            cosines[nid] = max(c)\n",
        "        \n",
        "        # print(cosines)\n",
        "        return max(cosines, key = cosines.get)\n",
        "    else:\n",
        "        return f\"{noun} not in Database\""
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4CkromrNqWM",
        "outputId": "2dd577fa-1268-440b-8e98-87fe87f27c04"
      },
      "source": [
        "test_sentence = \"अजमेर जिल्ह्याचे मुख्यालय अजमेर शहरात आहे.\"\n",
        "noun = \"अजमेर\"\n",
        "\n",
        "n = test_sentence.count(noun)\n",
        "\n",
        "for i in range(n):\n",
        "    print(getSense(test_sentence, noun, i+1))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "00025377\n",
            "00025377\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMj5ilCEJwet"
      },
      "source": [
        "word2vec_model1.save(\"word2vec_model.model\")"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN-7092ZfHOk"
      },
      "source": [
        "loaded_model = Word2Vec.load(\"/content/word2vec_model.model\")"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQvDhKT8ffHS"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao5sTAvS_ior"
      },
      "source": [
        "pickle.dump(noun_vectors,open(\"noun_vectors.pkl\", \"wb\"))"
      ],
      "execution_count": 77,
      "outputs": []
    }
  ]
}